{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wnCYXnwcxy1L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your JSON data\n",
        "with open(\"telegram_ecommerce_data_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "zy9v7Ml-x1-L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-compiled patterns for entity types\n",
        "price_patterns = [\n",
        "    re.compile(r\"(ዋጋ\\s*)?(\\d{1,3}(,\\d{3})*|\\d+)(\\s*ሚሊዮን|\\s*ብር)?\"),\n",
        "    re.compile(r\"(Price\\s*)(\\d{1,3}(,\\d{3})*|\\d+)(\\s*Million|\\s*birr)?\", re.IGNORECASE)\n",
        "]\n",
        "location_keywords = [\"አያት\", \"ቦሌ\", \"ሃዋሳ\", \"አዲስ\", \"ባህርዳር\", \"ጋለን\", \"ሀዋሳ\", \"ዋና\"]\n",
        "product_keywords = [\"መኪና\", \"Land\", \"Cruise\", \"Mark2\", \"ሌክሰስ\", \"ኮንዶሚንየም\", \"ሱቅ\", \"ቤት\", \"ሳሎን\", \"ማንኛውም\", \"ምርት\"]"
      ],
      "metadata": {
        "id": "D2CTicxRyMTl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_tokens(tokens):\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    # PRICE\n",
        "    joined = \" \".join(tokens)\n",
        "    for pattern in price_patterns:\n",
        "        for match in pattern.finditer(joined):\n",
        "            start, end = match.span()\n",
        "            matched_tokens = joined[start:end].split()\n",
        "            for i, token in enumerate(matched_tokens):\n",
        "                idx = find_token_index(tokens, token, i)\n",
        "                if idx != -1:\n",
        "                    labels[idx] = \"B-PRICE\" if i == 0 else \"I-PRICE\"\n",
        "\n",
        "    # LOCATION\n",
        "    for i, token in enumerate(tokens):\n",
        "        for loc in location_keywords:\n",
        "            if loc in token:\n",
        "                labels[i] = \"B-LOC\" if labels[i] == \"O\" else labels[i]\n",
        "\n",
        "    # PRODUCT\n",
        "    for i, token in enumerate(tokens):\n",
        "        for prod in product_keywords:\n",
        "            if prod in token:\n",
        "                labels[i] = \"B-Product\" if labels[i] == \"O\" else labels[i]\n",
        "\n",
        "    return list(zip(tokens, labels))"
      ],
      "metadata": {
        "id": "HJADWi9FyNtT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_token_index(tokens, word, start=0):\n",
        "    for i in range(start, len(tokens)):\n",
        "        if tokens[i] == word:\n",
        "            return i\n",
        "    return -1"
      ],
      "metadata": {
        "id": "t1JwT616yfNf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    # Basic whitespace and punctuation splitting\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "disyVeW7yibo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process and label first 50 messages\n",
        "output_lines = []\n",
        "for entry in itertools.islice(data, 50):\n",
        "    text = entry.get(\"clean_text\", \"\")\n",
        "    tokens = tokenize(text)\n",
        "    labeled = label_tokens(tokens)\n",
        "    for token, label in labeled:\n",
        "        output_lines.append(f\"{token} {label}\")\n",
        "    output_lines.append(\"\")  # Sentence separator"
      ],
      "metadata": {
        "id": "ED5uFeo0ym1d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saved output in CoNLL format\n",
        "with open(\"labeled_dataset.conll\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(output_lines))"
      ],
      "metadata": {
        "id": "f-V3TG10yoby"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}